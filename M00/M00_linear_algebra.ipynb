{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "622ce281",
   "metadata": {},
   "source": [
    "# Linear Algebra Essentials for LLMs from Scratch\n",
    "\n",
    "This notebook is designed to bridge the gap between basic linear algebra and its practical application in Large Language Models (LLMs).We will use PyTorch exclusively to build an intuitive understanding.\n",
    "\n",
    "\n",
    "## Why Linear Algebra Matters for LLMs\n",
    "\n",
    "Large Language Models are fundamentally:\n",
    "\n",
    "- Matrix multiplications\n",
    "- Vector projections\n",
    "- Linear transformations\n",
    "- Tensor reshaping\n",
    "\n",
    "Transformers are stacks of:\n",
    "- Linear layers (matrix multiplications)\n",
    "- Attention mechanisms (masked matrix multiplications)\n",
    "- Normalization layers (matrix addition)\n",
    "\n",
    "Everything reduces to linear algebra.\n",
    "\n",
    "Let’s build intuition step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cc42ec",
   "metadata": {},
   "source": [
    "# 1. Scalars, Vectors, and Matrices in PyTorch\n",
    "\n",
    "In neural networks:\n",
    "\n",
    "- Rank 0 →  **Scalar** → single number (e.g., loss value)\n",
    "- Rank 1 → **Vector** → 1D tensor (e.g., embedding of a token)\n",
    "- Rank 2 → **Matrix** → 2D tensor (e.g., weight matrix in Linear layer)\n",
    "- Rank 3+ → 3D Tensors (e.g., batch_size, seq_length, embedding_dim)\n",
    "\n",
    "In Transformers:\n",
    "- Embeddings are vectors\n",
    "- Weight parameters are matrices\n",
    "- Attention scores are matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e6af70-60b5-428d-a728-154cbbf50e90",
   "metadata": {},
   "source": [
    "## 1.1 Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f060489d-8430-427a-aa32-532212290223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar: tensor(3.1400)\n",
      "Scalar shape: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Scalar\n",
    "scalar = torch.tensor(3.14)\n",
    "print(\"Scalar:\", scalar)\n",
    "print(\"Scalar shape:\", scalar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e27d03c-1eca-4886-a363-f6f4f34828b1",
   "metadata": {},
   "source": [
    "## 1.2 Vectors\n",
    "\n",
    "A vector of length $n$, $\\mathbf{x}\\in\\mathbb{R}^{n}$, is a 1-dimensional (1-D) array of real numbers\n",
    "\n",
    "$$\n",
    "\\mathbf{x} =\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "When discussing vectors we will only consider column vectors. A row vector can always be obtained from a column vector via transposition\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{T} = [x_1, x_2, \\ldots, x_n].\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1200ce0f-a068-47ce-ac10-93fb20f9ccc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vector: tensor([1., 2., 3.])\n",
      "Vector shape: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Vector\n",
    "vector = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(\"\\nVector:\", vector)\n",
    "print(\"Vector shape:\", vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded88192-633a-4b66-9a9d-ae3fc9762b31",
   "metadata": {},
   "source": [
    "## 1.3 Matrix\n",
    "\n",
    "\n",
    "A matrix $A\\in\\mathbb{R}^{m\\times n}$ is a 2-D array of numbers\n",
    "\n",
    "$$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} &a_{m2} & \\cdots & a_{mn} \\\\\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "with $m$ rows and $n$ columns. The element at row $i$ and column $j$ is denoted $a_{ij}$. If $m=n$ we call it a square matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90b48372-4751-43c4-9ec1-c7c1b9346e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrix:\n",
      " tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "Matrix shape: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Matrix\n",
    "matrix = torch.tensor([[1.0, 2.0],\n",
    "                       [3.0, 4.0]])\n",
    "print(\"\\nMatrix:\\n\", matrix)\n",
    "print(\"Matrix shape:\", matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d1d44",
   "metadata": {},
   "source": [
    "# 2. Dot Product vs Element-wise Multiplication\n",
    "\n",
    "Understanding the difference between element-wise operations and matrix multiplication is crucial.\n",
    "\n",
    "- **Element-wise:** `*` operator. Shapes must match.\n",
    "- **Dot Product:** Sum of element-wise products (for vectors). Measures similarity between two vectors.\n",
    "\n",
    "\n",
    "In attention mechanisms, dot products compute similarity between queries and keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec763f3f-9096-4c63-9457-c24796012658",
   "metadata": {},
   "source": [
    "## 2.1 Element-wise Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fae70bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element-wise multiplication: tensor([ 4., 10., 18.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# Element-wise\n",
    "elementwise = a * b\n",
    "print(\"Element-wise multiplication:\", elementwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9784f765-08a7-45dd-a85e-841c7fa33a2f",
   "metadata": {},
   "source": [
    "## 2.2 Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b778f015-1826-492e-89c7-696076b02b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product: tensor(32.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# Dot product\n",
    "dot = torch.dot(a, b)\n",
    "print(\"Dot product:\", dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca696b-9713-46d7-84b5-60ac4fcaa100",
   "metadata": {},
   "source": [
    "# 3. Matrix Multiplication\n",
    "\n",
    "## 3.1 Matrix-scaler multiplication\n",
    "\n",
    "We can multiply matrices by scalar values and add matrices of the same dimension, i.e.,\n",
    "\n",
    "Let $c\\in\\mathbb{R}$ and $A\\in\\mathbb{R}^{m\\times n}$, then\n",
    "$$\n",
    "cA =\n",
    "\\begin{bmatrix}\n",
    "ca_{11} & ca_{12} & \\cdots & ca_{1n} \\\\\n",
    "ca_{21} & ca_{22} & \\cdots & ca_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "ca_{m1} & ca_{m2} & \\cdots & ca_{mn} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef67e18b-1d6c-4a9c-ac52-28579e8319ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix \n",
      " tensor([[-0.6443,  0.5578,  1.3072],\n",
      "        [-1.5412, -1.1672, -0.8862]])\n",
      "\n",
      " Scaled Matrix \n",
      " tensor([[ -64.4300,   55.7777,  130.7181],\n",
      "        [-154.1192, -116.7232,  -88.6225]])\n"
     ]
    }
   ],
   "source": [
    "c = 100\n",
    "A = torch.randn(2, 3)\n",
    "\n",
    "scaled_A = c*A\n",
    "print(\"Matrix \\n\", A)\n",
    "print(\"\\n Scaled Matrix \\n\", scaled_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45330ccb-4536-4c77-9c02-e71c510c7e09",
   "metadata": {},
   "source": [
    "\n",
    "## 3.2 Matrix-vector multiplication\n",
    "\n",
    "\n",
    "Let $A\\in\\mathbb{R}^{m\\times n}$ and $\\mathbf{x}\\in\\mathbb{R}^{n}$, then $A\\mathbf{x}\\in\\mathbb{R}^{m}$\n",
    "can be defined _row-wise_ as \n",
    "\n",
    "$$\n",
    "A\\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_1a_{11} + x_2 a_{12} + \\cdots + x_na_{1n} \\\\\n",
    "x_1a_{21} + x_2 a_{22} + \\cdots + x_na_{2n} \\\\\n",
    "\\vdots \\\\\n",
    "x_1a_{m1} + x_2 a_{m2} + \\cdots + x_na_{mn} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "081716c9-8ac9-4ee8-9069-beb688557cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix-vector multiplication A @ x:\n",
      "tensor([ 4.3928, -6.5343])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "result = A @ x \n",
    "print(f\"Matrix-vector multiplication A @ x:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c5a7a-ffd4-46f7-9f3c-3108f7fed7e7",
   "metadata": {},
   "source": [
    "Equivalently, this means that $A\\mathbf{x}$ is a linear combination of the _columns_ of $A$, i.e.,\n",
    "\n",
    "$$\n",
    "A\\mathbf{x} = \n",
    "x_1 \\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ \\vdots \\\\ a_{m1}  \\end{bmatrix} \n",
    "+ \n",
    "x_2  \\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{m2}  \\end{bmatrix}\n",
    "+\n",
    "\\cdots\n",
    "+\n",
    "x_n \\begin{bmatrix} a_{1n} \\\\ a_{2n} \\\\ \\vdots \\\\ a_{mn}  \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Observe that the matrix $A$ is a linear transformation that maps vectors in $\\mathbb{R}^{n}$ to $\\mathbb{R}^{m}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44f9b05a-6092-4833-9598-21989c9da45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As linear combination of columns:\n",
      "tensor([ 4.3928, -6.5343])\n"
     ]
    }
   ],
   "source": [
    "# Interpretation as linear combination of columns\n",
    "col1, col2, col3 = A[:, 0], A[:, 1], A[:, 2]\n",
    "linear_combination = x[0]*col1 + x[1]*col2 + x[2]*col3\n",
    "print(f\"As linear combination of columns:\\n{linear_combination}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a90bf8a",
   "metadata": {},
   "source": [
    "## 3.3 Matrix-matrix multiplication (is all you need)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1415d215-b817-4689-8fc4-2de68198dbe2",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00_matrix_multiplication_is_all_you_need.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcc90d5-fffc-4658-b322-93fe58d51860",
   "metadata": {},
   "source": [
    "**When you start digging into neural network layers and building your own, you'll find matrix multiplications everywhere.** Source: https://marksaroufim.substack.com/p/working-class-deep-learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bfb11f-a132-4e50-b82a-1ce2f444c514",
   "metadata": {},
   "source": [
    "Matrix multiplication is the core operation in any neural networks (not just transformers).\n",
    "\n",
    "PyTorch implements matrix multiplication functionality in the `torch.matmul()` method.\n",
    "\n",
    "The main two rules for matrix multiplication to remember are:\n",
    "\n",
    "The inner dimensions must match:\n",
    "- (3, 2) @ (3, 2) won't work\n",
    "- (2, 3) @ (3, 2) will work\n",
    "- (3, 2) @ (2, 3) will work\n",
    "  \n",
    "The resulting matrix has the shape of the outer dimensions:\n",
    "- (2, 3) @ (3, 2) -> (2, 2)\n",
    "- (3, 2) @ (2, 3) -> (3, 3)\n",
    "\n",
    "Note: One of the most common errors in deep learning (shape errors)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4579b-901b-4864-96e8-65ad31c1c2c2",
   "metadata": {},
   "source": [
    "In a Linear layer: `output = input @ weights`\n",
    "\n",
    "Where:\n",
    "- input shape: `(batch_size, input_dim)`\n",
    "- weights shape: `(input_dim, output_dim)`\n",
    "\n",
    "This is how embeddings are projected inside Transformers.\n",
    "\n",
    "Note: \"@\" in Python is the symbol for matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8356ae56-7a58-435d-a074-ed1d0490e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd7b483a-1896-4e78-a058-258372a8e18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 2])\n",
      "\n",
      "Output:\n",
      "tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n",
      "        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n",
      "        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Output shape: torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "651b3e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 2])\n",
      "Weight shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 2)  # batch of 3, 2 features\n",
    "W = torch.randn(2, 4)  # project to 4 features\n",
    "\n",
    "# torch.matmul(A, B) or torch.mm(A, B) or A @ B\n",
    "Y = x @ W\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Weight shape:\", W.shape)\n",
    "print(\"Output shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04258931-e789-427d-9d9d-3e47b1d1d7dc",
   "metadata": {},
   "source": [
    "The `torch.nn.Linear()` module, also known as a feed-forward layer or fully connected layer, implements a matrix multiplication between an input `x` and a weights matrix `A`.\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "- `x` is the input to the layer (deep learning is a stack of layers like torch.nn.Linear() and others on top of each other).\n",
    "- `A` is the weights matrix created by the layer, this starts out as random numbers that get adjusted as a neural network learns to better represent patterns in the data (notice the \"T\", that's because the weights matrix gets transposed).\n",
    "Note: You might also often see W or another letter like X used to showcase the weights matrix.\n",
    "- `b` is the bias term used to slightly offset the weights and inputs.\n",
    "- `y` is the output.\n",
    "This is a linear function (you may have seen something like $y = mx+b$ in high school or elsewhere), and can be used to draw a straight line!\n",
    "\n",
    "\n",
    "EX: Try changing the values of `in_features` and `out_features` below and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8166aaa8-a12d-45e6-9fea-6b1b2d232804",
   "metadata": {},
   "source": [
    "You can create your own matrix multiplication visuals at http://matrixmultiplication.xyz/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb60fb",
   "metadata": {},
   "source": [
    "# 4. Transpose and Backpropagation\n",
    "\n",
    "\n",
    "\n",
    "Transposing (`.T` or `torch.t()`) swaps rows and columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf6f3bb-8641-47a8-bb7c-58729e78067d",
   "metadata": {},
   "source": [
    "A matrix $A\\in\\mathbb{R}^{m\\times n}$ is a 2-D array of numbers\n",
    "\n",
    "$$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} &a_{m2} & \\cdots & a_{mn} \\\\\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "with $m$ rows and $n$ columns. The element at row $i$ and column $j$ is denoted $a_{ij}$. If $m=n$ we call it a square matrix.\n",
    "\n",
    "> Note: By convention, matrix indexing is opposite of graphical vector indexing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Transpose:** The transpose $A^{T}$ is defined as\n",
    "\n",
    "$$\n",
    "A^{T} = \n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{21} & \\cdots & a_{m1} \\\\\n",
    "a_{12} & a_{22} & \\cdots & a_{m2} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{1n} &a_{2n} & \\cdots & a_{nm} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The transpose turns columns of the matrix into rows (equivalently rows into columns). A square matrix is called symmetric if $A=A^{T}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0baf1-aa20-4a4d-9777-c57d67f4b469",
   "metadata": {},
   "source": [
    "**Why is this important?**\n",
    "1. **Dimension Alignment:** To perform matrix multiplication, inner dimensions must match.\n",
    "2. **Backpropagation:** When calculating gradients for weights, the input gradient must be transposed to match the weight shape.\n",
    "\n",
    "If W has shape (in, out), then W.T has shape (out, in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f918c742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([3, 4])\n",
      "Transposed shape: torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn(3, 4)\n",
    "print(\"Original shape:\", W.shape)\n",
    "\n",
    "W_T = W.T\n",
    "print(\"Transposed shape:\", W_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c269563-f571-411b-840c-1a5da42483df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs Shape: torch.Size([2, 4])\n",
      "Weights Shape: torch.Size([3, 4])\n",
      "Weights.T Shape: torch.Size([4, 3])\n",
      "Output Shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.randn(3, 4) # 3 output features, 4 input features\n",
    "inputs = torch.randn(2, 4)  # Batch of 2, 4 input features\n",
    "\n",
    "# Forward Pass: (Batch, Input) @ (Input, Output) -> (Batch, Output)\n",
    "# Note: weights usually needs to be transposed if defined as (Output, Input)\n",
    "output = inputs @ weights.T \n",
    "\n",
    "print(f\"Inputs Shape: {inputs.shape}\")\n",
    "print(f\"Weights Shape: {weights.shape}\")\n",
    "print(f\"Weights.T Shape: {weights.T.shape}\")\n",
    "print(f\"Output Shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de65073d",
   "metadata": {},
   "source": [
    "# 5. Special Matrices\n",
    "\n",
    "\n",
    "## Upper Triangular Matrix\n",
    "\n",
    "\n",
    "\n",
    "An upper triangular matrix $U\\in\\mathbb{R}^{n\\times n}$ is a matrix where all the entries below the main diagonal are zero\n",
    "\n",
    "$$\n",
    "U = \n",
    "\\begin{bmatrix}\n",
    "u_{11} & u_{12} & \\cdots & u_{1n} \\\\\n",
    "0 & u_{22} & \\cdots & u_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & u_{nn} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "**Why is this important?**\n",
    "1. Used for **causal masking** in self-attention (used in all decoder-only models like GPT, LLaMA).\n",
    "2. It prevents tokens from attending to future tokens.\n",
    "3. This is essential in GPT-style autoregressive models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0007ea78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 5\n",
    "\n",
    "mask_upper = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "print(mask_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694647db",
   "metadata": {},
   "source": [
    "## Lower Triangular Matrix\n",
    "\n",
    "\n",
    "\n",
    "A lower triangular matrix $L\\in\\mathbb{R}^{n\\times n}$ is a matrix where all the entries above the main diagonal are zero\n",
    "\n",
    "$$\n",
    "L =\n",
    "\\begin{bmatrix}\n",
    "l_{11} & 0 & \\cdots & 0 \\\\\n",
    "l_{12} & l_{22} & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "l_{1n} & l_{n2} & \\cdots & l_{nn} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ef79e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "mask_lower = torch.tril(torch.ones(seq_len, seq_len))\n",
    "print(mask_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5460d55",
   "metadata": {},
   "source": [
    "## Identity Matrix\n",
    "\n",
    "Acts like multiplication by 1.\n",
    "\n",
    "\n",
    "The $n \\times n$ identity matrix is\n",
    "\n",
    "$$\n",
    "\\mathbf{I} = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & \\cdots & 0 \\\\\n",
    "0 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & 1 \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "For every $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$, then $\\mathbf{AI} = \\mathbf{IA}$. \n",
    "\n",
    "In practice the identity matrix leaves any vector unchanged.  For example,\n",
    "$$\n",
    "\\begin{bmatrix}1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n",
    "\\begin{bmatrix}3\\\\ -2 \\end{bmatrix} = \\begin{bmatrix}3\\\\ -2 \\end{bmatrix}.\n",
    "$$\n",
    "Because of this property we sometimes call $I$ the *do nothing* transform.\n",
    "\n",
    "\n",
    "\n",
    "Useful in residual connections and initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a28e3e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "I = torch.eye(4)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b19b6d",
   "metadata": {},
   "source": [
    "## Diagonal Matrix\n",
    "\n",
    "A diagonal matrix $D\\in\\mathbb{R}^{n\\times n}$ has entries $d_{ij}=0$ if $i\\neq j$, i.e.,\n",
    "\n",
    "$$\n",
    "D =\n",
    "\\begin{bmatrix}\n",
    "d_{11} & 0 & \\cdots & 0 \\\\\n",
    "0 & d_{22} & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & d_{nn} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "Used in scaling operations.\n",
    "\n",
    "In attention, scaling by sqrt(d_k) is conceptually diagonal scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d8eddc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 2., 0.],\n",
      "        [0., 0., 3.]])\n"
     ]
    }
   ],
   "source": [
    "diag_values = torch.tensor([1.0, 2.0, 3.0])\n",
    "D = torch.diag(diag_values)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc04f12a",
   "metadata": {},
   "source": [
    "# Final Summary\n",
    "\n",
    "You now understand:\n",
    "\n",
    "- Scalars, vectors, matrices\n",
    "- Dot product vs element-wise multiplication\n",
    "- Matrix multiplication (is all you need)\n",
    "- Transpose in backpropagation\n",
    "- Triangular masks for causal self-attention\n",
    "- Identity and diagonal matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1cbfa6-5ddf-4c34-8fcd-7a752fb296f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
